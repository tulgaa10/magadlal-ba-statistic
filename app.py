"""
–ó–≠–≠–õ–ò–ô–ù –ë–ê–¢–õ–ê–ú–ñ–ò–ô–ù –¢–ê–ê–ú–ê–ì–õ–ê–õ (LOAN APPROVAL PREDICTION)
–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Ç”©—Å”©–ª

–≠—Ö —Å—É—Ä–≤–∞–ª–∂: Kaggle - Loan Approval Prediction Dataset
"""

# ======================================================================
# 1. –®–ê–ê–†–î–õ–ê–ì–ê–¢–ê–ô –°–ê–ù–ì–£–£–î –¢–ê–¢–ê–•
# ======================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (accuracy_score, classification_report, 
                            confusion_matrix, roc_curve, auc, 
                            precision_recall_curve, f1_score, roc_auc_score)
from sklearn import tree
import warnings
warnings.filterwarnings('ignore')

# –ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç–º–∂–∏—Ö —Ç–æ—Ö–∏—Ä–≥–æ–æ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 80)
print("–ó–≠–≠–õ–ò–ô–ù –ë–ê–¢–õ–ê–ú–ñ–ò–ô–ù –¢–ê–ê–ú–ê–ì–õ–ê–õ - –ú–ê–®–ò–ù –°–£–†–ì–ê–õ–¢–´–ù –¢”®–°”®–õ")
print("=" * 80)

# ======================================================================
# 2. ”®–ì”®–ì–î”®–õ –£–ù–®–ò–ñ –ê–í–ê–•
# ======================================================================

print("\nüìä –ê–õ–•–ê–ú 1: ”®–ì”®–ì–î”®–õ –£–ù–®–ò–ñ –ê–í–ê–•")
print("-" * 80)

try:
    # ”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –∞–≤–∞—Ö
    train_df = pd.read_csv('loan_train.csv')
    test_df = pd.read_csv('loan_test.csv')
    
    print(f"‚úì –°—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª —Ç–∞—Ç–∞–≥–¥–ª–∞–∞: {train_df.shape[0]} –º”©—Ä, {train_df.shape[1]} –±–∞–≥–∞–Ω–∞")
    print(f"‚úì –¢–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥”©–ª —Ç–∞—Ç–∞–≥–¥–ª–∞–∞: {test_df.shape[0]} –º”©—Ä, {test_df.shape[1]} –±–∞–≥–∞–Ω–∞")
    
    print(f"\nüìã ”®–≥”©–≥–¥–ª–∏–π–Ω —ç—Ö–Ω–∏–π 5 –º”©—Ä:")
    print(train_df.head())
    
    print(f"\nüìã –ë–∞–≥–∞–Ω—É—É–¥—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç:")
    for i, col in enumerate(train_df.columns, 1):
        print(f"  {i}. {col:30} - {train_df[col].dtype}")
    
except FileNotFoundError:
    print("‚ùå –ê–õ–î–ê–ê: loan_train.csv —ç—Å–≤—ç–ª loan_test.csv —Ñ–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π!")
    print("   –§–∞–π–ª—É—É–¥—ã–≥ Python —Å–∫—Ä–∏–ø—Ç–∏–π–Ω —Ö–∞–∂—É—É–¥ –±–∞–π—Ä–ª—É—É–ª–Ω–∞ —É—É.")
    exit()

# ======================================================================
# 3. ”®–ì”®–ì–î–õ–ò–ô–ù –ê–ù–ê–õ–ò–ó
# ======================================================================

print("\n" + "=" * 80)
print("üìä –ê–õ–•–ê–ú 2: ”®–ì”®–ì–î–õ–ò–ô–ù –ê–ù–ê–õ–ò–ó")
print("-" * 80)

print("\nüìå ”®–≥”©–≥–¥–ª–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª:")
print(train_df.info())

print("\nüìå –°—Ç–∞—Ç–∏—Å—Ç–∏–∫ “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥ (—Ç–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–∏–¥):")
print(train_df.describe())

print("\nüìå –ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—É—É–¥:")
missing = train_df.isnull().sum()
missing_pct = (missing / len(train_df) * 100).round(2)
missing_df = pd.DataFrame({
    '–ë–∞–≥–∞–Ω–∞': missing.index,
    '–ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —Ç–æ–æ': missing.values,
    '–•—É–≤—å (%)': missing_pct.values
})
missing_df = missing_df[missing_df['–ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —Ç–æ–æ'] > 0].sort_values('–ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —Ç–æ–æ', ascending=False)
if len(missing_df) > 0:
    print(missing_df.to_string(index=False))
else:
    print("–ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥–∞ –±–∞–π—Ö–≥“Ø–π")

# –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ –æ–ª–æ—Ö (Loan_Status –±—É—é—É —Ç”©—Å—Ç—ç–π –Ω—ç—Ä—Ç—ç–π)
target_col = None
for col in train_df.columns:
    if 'status' in col.lower() or 'approval' in col.lower() or 'loan_status' in col.lower():
        target_col = col
        break

if target_col is None:
    # –°“Ø“Ø–ª–∏–π–Ω –±–∞–≥–∞–Ω—ã–≥ –∑–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á –≥—ç–∂ “Ø–∑—ç—Ö
    target_col = train_df.columns[-1]
    print(f"\n‚ö†Ô∏è –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —Å–æ–Ω–≥–æ—Å–æ–Ω: {target_col}")

print(f"\nüìå –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á: {target_col}")
print(f"üìå –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç:")
if train_df[target_col].dtype == 'object':
    status_counts = train_df[target_col].value_counts()
    print(status_counts)
    for status, count in status_counts.items():
        pct = count / len(train_df) * 100
        print(f"  {status}: {count} ({pct:.1f}%)")
else:
    print(train_df[target_col].value_counts())

# ======================================================================
# 4. ”®–ì”®–ì–î”®–õ –¶–≠–í–≠–†–õ–≠–• –ë–ê –ë–û–õ–û–í–°–†–£–£–õ–ê–õ–¢
# ======================================================================

print("\n" + "=" * 80)
print("‚öôÔ∏è –ê–õ–•–ê–ú 3: ”®–ì”®–ì–î”®–õ –¶–≠–í–≠–†–õ–≠–• –ë–ê –ë–û–õ–û–í–°–†–£–£–õ–ê–õ–¢")
print("-" * 80)

# ”®–≥”©–≥–¥–ª–∏–π–Ω —Ö—É—É–ª–±–∞—Ä “Ø“Ø—Å–≥—ç—Ö
df = train_df.copy()

# ID –±–∞–≥–∞–Ω—ã–≥ —É—Å—Ç–≥–∞—Ö (—Ö—ç—Ä—ç–≤ –±–∞–π–≥–∞–∞ –±–æ–ª)
id_cols = [col for col in df.columns if 'id' in col.lower()]
if id_cols:
    print(f"\n‚úì ID –±–∞–≥–∞–Ω–∞(—É—É–¥) —É—Å—Ç–≥–∞–≥–¥–ª–∞–∞: {id_cols}")
    df = df.drop(columns=id_cols)

# –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ —Ç–æ–æ–Ω –±–æ–ª–≥–æ—Ö
if df[target_col].dtype == 'object':
    le_target = LabelEncoder()
    df[target_col] = le_target.fit_transform(df[target_col])
    print(f"\n‚úì –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á –∫–æ–¥–ª–æ–≥–¥–ª–æ–æ:")
    for i, label in enumerate(le_target.classes_):
        print(f"  {label} ‚Üí {i}")

# –ö–∞—Ç–µ–≥–æ—Ä–∏ –±–æ–ª–æ–Ω —Ç–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–¥—ã–≥ —è–ª–≥–∞—Ö
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ —Ç–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–¥–∞–∞—Å —Ö–∞—Å–∞—Ö
if target_col in numeric_cols:
    numeric_cols.remove(target_col)

print(f"\nüìä –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–∏–¥ ({len(categorical_cols)}):")
for col in categorical_cols:
    unique_count = df[col].nunique()
    print(f"  ‚Ä¢ {col}: {unique_count} ”©”©—Ä —É—Ç–≥–∞")

print(f"\nüìä –¢–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–∏–¥ ({len(numeric_cols)}):")
for col in numeric_cols:
    print(f"  ‚Ä¢ {col}")

# –ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—É—É–¥—ã–≥ –±”©–≥–ª”©—Ö
print("\nüîß –ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—É—É–¥—ã–≥ –±”©–≥–ª”©–∂ –±–∞–π–Ω–∞...")

# –¢–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—ã–≥ –¥—É–Ω–¥–∞–∂ —É—Ç–≥–∞–∞—Ä –±”©–≥–ª”©—Ö
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"  ‚úì {col}: –¥—É–Ω–¥–∞–∂ —É—Ç–≥–∞–∞—Ä ({median_val:.2f})")

# –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—ã–≥ –º–æ–¥–∞–∞—Ä (—Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª —É—Ç–≥–∞) –±”©–≥–ª”©—Ö
for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_val = df[col].mode()[0]
        df[col].fillna(mode_val, inplace=True)
        print(f"  ‚úì {col}: –º–æ–¥–∞–∞—Ä ({mode_val})")

print("\n‚úì –ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—É—É–¥ –±“Ø–≥–¥ –±”©–≥–ª”©–≥–¥–ª”©”©")

# –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–≥ –∫–æ–¥–ª–æ—Ö
print("\nüîß –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–≥ –∫–æ–¥–ª–æ–∂ –±–∞–π–Ω–∞...")
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
    print(f"  ‚úì {col}: {len(le.classes_)} –∫–∞—Ç–µ–≥–æ—Ä–∏ –∫–æ–¥–ª–æ–≥–¥–ª–æ–æ")

# ======================================================================
# 5. ”®–ì”®–ì–î–õ–ò–ô–ù –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò
# ======================================================================

print("\n" + "=" * 80)
print("üìà –ê–õ–•–ê–ú 4: ”®–ì”®–ì–î–õ–ò–ô–ù –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò")
print("-" * 80)

# –ì—Ä–∞—Ñ–∏–∫ 1: –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á + —Ç–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ç–∞—Ä—Ö–∞–ª—Ç
n_plots = min(6, len(numeric_cols) + 1)
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

# –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç
status_counts = df[target_col].value_counts()
colors_map = ['#2ecc71', '#e74c3c']
axes[0].bar(range(len(status_counts)), status_counts.values, color=colors_map[:len(status_counts)])
axes[0].set_title(f'{target_col} - –¢–∞—Ä—Ö–∞–ª—Ç', fontsize=12, fontweight='bold')
axes[0].set_xlabel('–ê–Ω–≥–∏–ª–∞–ª')
axes[0].set_ylabel('–¢–æ–æ')
axes[0].set_xticks(range(len(status_counts)))
if 'le_target' in locals():
    axes[0].set_xticklabels(le_target.classes_, rotation=0)

# –¢–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ç–∞—Ä—Ö–∞–ª—Ç
plot_colors = ['#3498db', '#e67e22', '#9b59b6', '#1abc9c', '#f39c12']
for idx, col in enumerate(numeric_cols[:5]):
    axes[idx+1].hist(df[col].dropna(), bins=30, color=plot_colors[idx % len(plot_colors)], 
                     edgecolor='black', alpha=0.7)
    axes[idx+1].set_title(f'{col} - –¢–∞—Ä—Ö–∞–ª—Ç', fontsize=10, fontweight='bold')
    axes[idx+1].set_xlabel(col)
    axes[idx+1].set_ylabel('–î–∞–≤—Ç–∞–º–∂')

plt.tight_layout()
plt.savefig('loan_distributions.png', dpi=300, bbox_inches='tight')
print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: loan_distributions.png")
plt.close()

# –ì—Ä–∞—Ñ–∏–∫ 2: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü
print("\nüîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –∞–Ω–∞–ª–∏–∑ —Ö–∏–π–∂ –±–∞–π–Ω–∞...")
correlation_matrix = df.corr()

plt.figure(figsize=(14, 12))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, mask=mask,
            cbar_kws={"shrink": 0.8})
plt.title('–•—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∫–æ—Ä—Ä–µ–ª—è—Ü–∏', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: correlation_matrix.png")
plt.close()

print(f"\nüìå {target_col}-—Ç–∞–π —Ö–∞–º–≥–∏–π–Ω –∏—Ö —Ö–æ–ª–±–æ–æ—Ç–æ–π —Ö—É–≤—å—Å–∞–≥—á–∏–¥:")
target_corr = correlation_matrix[target_col].sort_values(ascending=False)
print(target_corr.head(10))

# –ì—Ä–∞—Ñ–∏–∫ 3: –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á—Ç–∞–π —Ö–∞–º–≥–∏–π–Ω –∏—Ö —Ö–æ–ª–±–æ–æ—Ç–æ–π —Ö—É–≤—å—Å–∞–≥—á–∏–¥
top_features = target_corr.abs().sort_values(ascending=False)[1:6]
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

for idx, (feature, corr_val) in enumerate(top_features.items()):
    if feature in df.columns:
        # Box plot
        df.boxplot(column=feature, by=target_col, ax=axes[idx])
        axes[idx].set_title(f'{feature}\n(–ö–æ—Ä—Ä–µ–ª—è—Ü–∏: {corr_val:.3f})', fontweight='bold')
        axes[idx].set_xlabel('')
        plt.sca(axes[idx])
        plt.xticks(range(1, len(status_counts)+1), 
                  le_target.classes_ if 'le_target' in locals() else range(len(status_counts)))

# –°“Ø“Ø–ª–∏–π–Ω —Ö–æ–æ—Å–æ–Ω –≥—Ä–∞—Ñ–∏–∫–∏–π–≥ –Ω—É—É—Ö
if len(top_features) < 6:
    for idx in range(len(top_features), 6):
        axes[idx].axis('off')

plt.suptitle('–ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á—Ç–∞–π —Ö–∞–º–≥–∏–π–Ω –∏—Ö —Ö–æ–ª–±–æ–æ—Ç–æ–π —Ö—É–≤—å—Å–∞–≥—á–∏–¥', 
             fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('feature_relationships.png', dpi=300, bbox_inches='tight')
print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: feature_relationships.png")
plt.close()

# ======================================================================
# 6. ”®–ì”®–ì–î–õ–ò–ô–ì –°–£–†–ì–ê–õ–¢ –ë–ê –¢–ï–°–¢–≠–î –•–£–í–ê–ê–•
# ======================================================================

print("\n" + "=" * 80)
print("‚úÇÔ∏è –ê–õ–•–ê–ú 5: ”®–ì”®–ì–î–õ–ò–ô–ì –•–£–í–ê–ê–•")
print("-" * 80)

# X (—à–∏–Ω–∂) –±–æ–ª–æ–Ω y (–∑–æ—Ä–∏–ª—Ç–æ—Ç) —Å–∞–ª–≥–∞—Ö
X = df.drop(target_col, axis=1)
y = df[target_col]

print(f"‚úì –®–∏–Ω–∂–∏–π–Ω —Ç–æ–æ: {X.shape[1]}")
print(f"‚úì ”®–≥”©–≥–¥–ª–∏–π–Ω —Ç–æ–æ: {X.shape[0]}")

# –°—É—Ä–≥–∞–ª—Ç –±–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–π–Ω ”©–≥”©–≥–¥”©–ª–¥ —Ö—É–≤–∞–∞—Ö (80-20)
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\n‚úì –°—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏–π–Ω ”©–≥”©–≥–¥”©–ª: {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.1f}%)")

# –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–ª–∞—Ö
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

print("\n‚úì ”®–≥”©–≥–¥”©–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç—á–ª–∞–≥–¥–ª–∞–∞ (StandardScaler)")

# ======================================================================
# 7. –ú–ê–®–ò–ù –°–£–†–ì–ê–õ–¢–´–ù –ó–ê–ì–í–ê–†–£–£–î
# ======================================================================

print("\n" + "=" * 80)
print("ü§ñ –ê–õ–•–ê–ú 6: –ú–ê–®–ò–ù –°–£–†–ì–ê–õ–¢–´–ù –ó–ê–ì–í–ê–†–£–£–î")
print("=" * 80)

# –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö–∞–¥–≥–∞–ª–∞—Ö
results = {}
predictions = {}
models = {}

# --------------------------------------------------
# 7.1. –õ–û–ì–ò–°–¢–ò–ö –†–ï–ì–†–ï–°–°
# --------------------------------------------------
print("\n" + "-" * 80)
print("1Ô∏è‚É£ –õ–û–ì–ò–°–¢–ò–ö –†–ï–ì–†–ï–°–° (LOGISTIC REGRESSION)")
print("-" * 80)

log_reg = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')
log_reg.fit(X_train_scaled, y_train)
y_pred_log = log_reg.predict(X_val_scaled)
y_pred_proba_log = log_reg.predict_proba(X_val_scaled)[:, 1]

acc_log = accuracy_score(y_val, y_pred_log)
f1_log = f1_score(y_val, y_pred_log, average='weighted')
auc_log = roc_auc_score(y_val, y_pred_proba_log) if len(np.unique(y)) == 2 else 0

# Cross-validation
cv_scores_log = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='accuracy')

results['–õ–æ–≥–∏—Å—Ç–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å'] = {
    'accuracy': acc_log, 
    'f1_score': f1_log,
    'auc': auc_log,
    'cv_mean': cv_scores_log.mean(),
    'cv_std': cv_scores_log.std()
}
predictions['–õ–æ–≥–∏—Å—Ç–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å'] = (y_pred_log, y_pred_proba_log)
models['–õ–æ–≥–∏—Å—Ç–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å'] = log_reg

print(f"‚úì –ù–∞—Ä–∏–π–≤—á–ª–∞–ª (Accuracy): {acc_log:.4f}")
print(f"‚úì F1-Score: {f1_log:.4f}")
if len(np.unique(y)) == 2:
    print(f"‚úì AUC: {auc_log:.4f}")
print(f"‚úì Cross-Validation: {cv_scores_log.mean():.4f} (¬±{cv_scores_log.std():.4f})")

print("\nüìä –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–∞–Ω:")
print(classification_report(y_val, y_pred_log, 
                          target_names=le_target.classes_ if 'le_target' in locals() else None))

# --------------------------------------------------
# 7.2. –®–ò–ô–î–í–≠–†–ò–ô–ù –ú–û–î
# --------------------------------------------------
print("\n" + "-" * 80)
print("2Ô∏è‚É£ –®–ò–ô–î–í–≠–†–ò–ô–ù –ú–û–î (DECISION TREE)")
print("-" * 80)

dt_model = DecisionTreeClassifier(random_state=42, max_depth=8, 
                                  min_samples_split=20, min_samples_leaf=10)
dt_model.fit(X_train_scaled, y_train)
y_pred_dt = dt_model.predict(X_val_scaled)
y_pred_proba_dt = dt_model.predict_proba(X_val_scaled)[:, 1]

acc_dt = accuracy_score(y_val, y_pred_dt)
f1_dt = f1_score(y_val, y_pred_dt, average='weighted')
auc_dt = roc_auc_score(y_val, y_pred_proba_dt) if len(np.unique(y)) == 2 else 0

cv_scores_dt = cross_val_score(dt_model, X_train_scaled, y_train, cv=5, scoring='accuracy')

results['–®–∏–π–¥–≤—ç—Ä–∏–π–Ω –º–æ–¥'] = {
    'accuracy': acc_dt, 
    'f1_score': f1_dt,
    'auc': auc_dt,
    'cv_mean': cv_scores_dt.mean(),
    'cv_std': cv_scores_dt.std()
}
predictions['–®–∏–π–¥–≤—ç—Ä–∏–π–Ω –º–æ–¥'] = (y_pred_dt, y_pred_proba_dt)
models['–®–∏–π–¥–≤—ç—Ä–∏–π–Ω –º–æ–¥'] = dt_model

print(f"‚úì –ù–∞—Ä–∏–π–≤—á–ª–∞–ª (Accuracy): {acc_dt:.4f}")
print(f"‚úì F1-Score: {f1_dt:.4f}")
if len(np.unique(y)) == 2:
    print(f"‚úì AUC: {auc_dt:.4f}")
print(f"‚úì Cross-Validation: {cv_scores_dt.mean():.4f} (¬±{cv_scores_dt.std():.4f})")

print("\nüìä –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–∞–Ω:")
print(classification_report(y_val, y_pred_dt,
                          target_names=le_target.classes_ if 'le_target' in locals() else None))

# –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª
feature_importance_dt = pd.DataFrame({
    'feature': X.columns,
    'importance': dt_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nüìå –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª (—ç—Ö–Ω–∏–π 10):")
print(feature_importance_dt.head(10).to_string(index=False))

# --------------------------------------------------
# 7.3. RANDOM FOREST
# --------------------------------------------------
print("\n" + "-" * 80)
print("3Ô∏è‚É£ RANDOM FOREST")
print("-" * 80)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, 
                                 max_depth=10, min_samples_split=20,
                                 min_samples_leaf=10, n_jobs=-1)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_val_scaled)
y_pred_proba_rf = rf_model.predict_proba(X_val_scaled)[:, 1]

acc_rf = accuracy_score(y_val, y_pred_rf)
f1_rf = f1_score(y_val, y_pred_rf, average='weighted')
auc_rf = roc_auc_score(y_val, y_pred_proba_rf) if len(np.unique(y)) == 2 else 0

cv_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy')

results['Random Forest'] = {
    'accuracy': acc_rf, 
    'f1_score': f1_rf,
    'auc': auc_rf,
    'cv_mean': cv_scores_rf.mean(),
    'cv_std': cv_scores_rf.std()
}
predictions['Random Forest'] = (y_pred_rf, y_pred_proba_rf)
models['Random Forest'] = rf_model

print(f"‚úì –ù–∞—Ä–∏–π–≤—á–ª–∞–ª (Accuracy): {acc_rf:.4f}")
print(f"‚úì F1-Score: {f1_rf:.4f}")
if len(np.unique(y)) == 2:
    print(f"‚úì AUC: {auc_rf:.4f}")
print(f"‚úì Cross-Validation: {cv_scores_rf.mean():.4f} (¬±{cv_scores_rf.std():.4f})")

print("\nüìä –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–∞–Ω:")
print(classification_report(y_val, y_pred_rf,
                          target_names=le_target.classes_ if 'le_target' in locals() else None))

# –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª
feature_importance_rf = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nüìå –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª (—ç—Ö–Ω–∏–π 10):")
print(feature_importance_rf.head(10).to_string(index=False))

# --------------------------------------------------
# 7.4. NAIVE BAYES
# --------------------------------------------------
print("\n" + "-" * 80)
print("4Ô∏è‚É£ NAIVE BAYES")
print("-" * 80)

nb_model = GaussianNB()
nb_model.fit(X_train_scaled, y_train)
y_pred_nb = nb_model.predict(X_val_scaled)
y_pred_proba_nb = nb_model.predict_proba(X_val_scaled)[:, 1]

acc_nb = accuracy_score(y_val, y_pred_nb)
f1_nb = f1_score(y_val, y_pred_nb, average='weighted')
auc_nb = roc_auc_score(y_val, y_pred_proba_nb) if len(np.unique(y)) == 2 else 0

cv_scores_nb = cross_val_score(nb_model, X_train_scaled, y_train, cv=5, scoring='accuracy')

results['Naive Bayes'] = {
    'accuracy': acc_nb, 
    'f1_score': f1_nb,
    'auc': auc_nb,
    'cv_mean': cv_scores_nb.mean(),
    'cv_std': cv_scores_nb.std()
}
predictions['Naive Bayes'] = (y_pred_nb, y_pred_proba_nb)
models['Naive Bayes'] = nb_model

print(f"‚úì –ù–∞—Ä–∏–π–≤—á–ª–∞–ª (Accuracy): {acc_nb:.4f}")
print(f"‚úì F1-Score: {f1_nb:.4f}")
if len(np.unique(y)) == 2:
    print(f"‚úì AUC: {auc_nb:.4f}")
print(f"‚úì Cross-Validation: {cv_scores_nb.mean():.4f} (¬±{cv_scores_nb.std():.4f})")

print("\nüìä –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–∞–Ω:")
print(classification_report(y_val, y_pred_nb,
                          target_names=le_target.classes_ if 'le_target' in locals() else None))

# ======================================================================
# 8. –ó–ê–ì–í–ê–†–£–£–î–´–ù –•–ê–†–¨–¶–£–£–õ–ê–õ–¢
# ======================================================================

print("\n" + "=" * 80)
print("üìä –ê–õ–•–ê–ú 7: –ó–ê–ì–í–ê–†–£–£–î–´–ù –•–ê–†–¨–¶–£–£–õ–ê–õ–¢")
print("=" * 80)

# “Æ—Ä –¥“Ø–Ω–≥–∏–π–Ω —Ö“Ø—Å–Ω—ç–≥—Ç
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('accuracy', ascending=False)

print("\nüìå –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω “Ø—Ä –¥“Ø–Ω–≥–∏–π–Ω —Ö“Ø—Å–Ω—ç–≥—Ç:")
print(results_df.to_string())
print(f"\nüèÜ –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∑–∞–≥–≤–∞—Ä: {results_df.index[0]} "
      f"(Accuracy: {results_df.iloc[0]['accuracy']:.4f})")

# –•–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω –≥—Ä–∞—Ñ–∏–∫
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Accuracy —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
axes[0, 0].barh(results_df.index, results_df['accuracy'], color='steelblue')
axes[0, 0].set_xlabel('–ù–∞—Ä–∏–π–≤—á–ª–∞–ª (Accuracy)', fontweight='bold')
axes[0, 0].set_title('–ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª', fontsize=14, fontweight='bold')
axes[0, 0].set_xlim(0, 1)
for i, v in enumerate(results_df['accuracy']):
    axes[0, 0].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')

# F1-Score —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
axes[0, 1].barh(results_df.index, results_df['f1_score'], color='coral')
axes[0, 1].set_xlabel('F1-Score', fontweight='bold')
axes[0, 1].set_title('–ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω F1-Score', fontsize=14, fontweight='bold')
axes[0, 1].set_xlim(0, 1)
for i, v in enumerate(results_df['f1_score']):
    axes[0, 1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')

# AUC —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
if len(np.unique(y)) == 2:
    axes[1, 0].barh(results_df.index, results_df['auc'], color='mediumpurple')
    axes[1, 0].set_xlabel('AUC Score', fontweight='bold')
    axes[1, 0].set_title('–ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω AUC', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlim(0, 1)
    for i, v in enumerate(results_df['auc']):
        axes[1, 0].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')
else:
    axes[1, 0].axis('off')

# Cross-validation —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
axes[1, 1].barh(results_df.index, results_df['cv_mean'], color='lightgreen')
axes[1, 1].set_xlabel('Cross-Validation Score', fontweight='bold')
axes[1, 1].set_title('Cross-Validation (5-fold)', fontsize=14, fontweight='bold')
axes[1, 1].set_xlim(0, 1)
for i, v in enumerate(results_df['cv_mean']):
    axes[1, 1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
print("\n‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: model_comparison.png")
plt.close()

# ======================================================================
# 9. ROC –ú–£–†–£–ô –ë–ê AUC
# ======================================================================

if len(np.unique(y)) == 2:
    print("\n" + "=" * 80)
    print("üìà –ê–õ–•–ê–ú 8: ROC –ú–£–†–£–ô –ë–ê AUC")
    print("=" * 80)

    plt.figure(figsize=(10, 8))

    colors = ['blue', 'green', 'red', 'purple']
    for (name, (_, y_pred_proba)), color in zip(predictions.items(), colors):
        fpr, tpr, _ = roc_curve(y_val, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2, 
                 label=f'{name} (AUC = {roc_auc:.3f})')
        print(f"‚úì {name}: AUC = {roc_auc:.4f}")

    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='–°–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π (AUC = 0.500)')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontweight='bold', fontsize=12)
    plt.ylabel('True Positive Rate', fontweight='bold', fontsize=12)
    plt.title('ROC –ú—É—Ä—É–π - –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
    print("\n‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: roc_curves.png")
    plt.close()

# ======================================================================
# 10. CONFUSION MATRIX
# ======================================================================

print("\n" + "=" * 80)
print("üéØ –ê–õ–•–ê–ú 9: CONFUSION MATRIX")
print("=" * 80)

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, (name, (y_pred, _)) in enumerate(predictions.items()):
    cm = confusion_matrix(y_val, y_pred)
    
    labels = le_target.classes_ if 'le_target' in locals() else [str(i) for i in range(len(np.unique(y)))]
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                xticklabels=labels, yticklabels=labels,
                cbar_kws={'label': '–¢–æ–æ'})
    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')
    axes[idx].set_ylabel('–ë–æ–¥–∏—Ç —É—Ç–≥–∞', fontweight='bold')
    axes[idx].set_xlabel('–¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥–∞', fontweight='bold')
    
    # –ù–∞—Ä–∏–π–≤—á–ª–∞–ª –Ω—ç–º–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
    acc = accuracy_score(y_val, y_pred)
    axes[idx].text(0.5, -0.15, f'Accuracy: {acc:.4f}', 
                   transform=axes[idx].transAxes, ha='center',
                   fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: confusion_matrices.png")
plt.close()

# ======================================================================
# 11. –•–£–í–¨–°–ê–ì–ß–î–´–ù –ê–ß –•–û–õ–ë–û–ì–î–û–õ
# ======================================================================

print("\n" + "=" * 80)
print("üîç –ê–õ–•–ê–ú 10: –•–£–í–¨–°–ê–ì–ß–î–´–ù –ê–ß –•–û–õ–ë–û–ì–î–û–õ")
print("=" * 80)

# Random Forest-–∏–π–Ω —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–ª—ã–≥ –≥—Ä–∞—Ñ–∏–∫ –±–æ–ª–≥–æ—Ö
plt.figure(figsize=(12, 8))
top_n = min(15, len(feature_importance_rf))
top_features_rf = feature_importance_rf.head(top_n)

plt.barh(range(len(top_features_rf)), top_features_rf['importance'], 
         color='steelblue', edgecolor='black')
plt.yticks(range(len(top_features_rf)), top_features_rf['feature'])
plt.xlabel('–ê—á —Ö–æ–ª–±–æ–≥–¥–æ–ª', fontweight='bold', fontsize=12)
plt.ylabel('–•—É–≤—å—Å–∞–≥—á', fontweight='bold', fontsize=12)
plt.title(f'–•–∞–º–≥–∏–π–Ω —á—É—Ö–∞–ª {top_n} —Ö—É–≤—å—Å–∞–≥—á (Random Forest)', 
          fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
print("‚úì –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: feature_importance.png")
plt.close()

# ======================================================================
# 12. –¢–ï–°–¢–ò–ô–ù ”®–ì”®–ì–î”®–õ –î–≠–≠–† –¢–ê–ê–ú–ê–ì–õ–ê–õ –•–ò–ô–•
# ======================================================================

print("\n" + "=" * 80)
print("üîÆ –ê–õ–•–ê–ú 11: –¢–ï–°–¢–ò–ô–ù ”®–ì”®–ì–î”®–õ –î–≠–≠–† –¢–ê–ê–ú–ê–ì–õ–ê–õ")
print("=" * 80)

# –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∑–∞–≥–≤–∞—Ä—ã–≥ —Å–æ–Ω–≥–æ—Ö
best_model_name = results_df.index[0]
best_model = models[best_model_name]

print(f"‚úì –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∑–∞–≥–≤–∞—Ä: {best_model_name}")
print(f"‚úì Validation accuracy: {results_df.iloc[0]['accuracy']:.4f}")

# –¢–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥–ª–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö
test_df_processed = test_df.copy()

# ID –±–∞–≥–∞–Ω–∞ —Ö–∞–¥–≥–∞–ª–∞—Ö (—Ö—ç—Ä—ç–≤ –±–∞–π–≥–∞–∞ –±–æ–ª)
test_ids = None
for col in test_df.columns:
    if 'id' in col.lower():
        test_ids = test_df_processed[col].copy()
        test_df_processed = test_df_processed.drop(columns=[col])
        break

# –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
if target_col in test_df_processed.columns:
    test_df_processed = test_df_processed.drop(columns=[target_col])
    print(f"‚ö†Ô∏è –¢–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥–ª”©”©—Å {target_col} —É—Å—Ç–≥–∞–≥–¥–ª–∞–∞")

# –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–≥ –∫–æ–¥–ª–æ—Ö
for col in categorical_cols:
    if col in test_df_processed.columns:
        if col in label_encoders:
            # –®–∏–Ω—ç –∫–∞—Ç–µ–≥–æ—Ä–∏ –∏–ª—ç—Ä–≤—ç–ª —Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª —É—Ç–≥—ã–≥ ”©–≥”©—Ö
            le = label_encoders[col]
            def safe_transform(x):
                if x in le.classes_:
                    return le.transform([x])[0]
                else:
                    return le.transform([le.classes_[0]])[0]
            test_df_processed[col] = test_df_processed[col].apply(safe_transform)

# –ê–ª–≥–∞ –±–æ–ª—Å–æ–Ω —É—Ç–≥—É—É–¥—ã–≥ –±”©–≥–ª”©—Ö
for col in numeric_cols:
    if col in test_df_processed.columns:
        if test_df_processed[col].isnull().sum() > 0:
            median_val = df[col].median()
            test_df_processed[col].fillna(median_val, inplace=True)

for col in categorical_cols:
    if col in test_df_processed.columns:
        if test_df_processed[col].isnull().sum() > 0:
            mode_val = 0  # –ö–æ–¥–ª–æ–≥–¥—Å–æ–Ω —É—Ç–≥–∞
            test_df_processed[col].fillna(mode_val, inplace=True)

# –ë–∞–≥–∞–Ω—É—É–¥—ã–≥ —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö
for col in X.columns:
    if col not in test_df_processed.columns:
        test_df_processed[col] = 0

test_df_processed = test_df_processed[X.columns]

# –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–ª–∞—Ö
X_test_scaled = scaler.transform(test_df_processed)

# –¢–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π—Ö
test_predictions = best_model.predict(X_test_scaled)
test_predictions_proba = best_model.predict_proba(X_test_scaled)

print(f"\n‚úì {len(test_predictions)} —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–≥–¥–ª—ç—ç")

# –¢–∞–∞–º–∞–≥–ª–∞–ª—ã–Ω —Ç–∞—Ä—Ö–∞–ª—Ç
pred_counts = pd.Series(test_predictions).value_counts().sort_index()
print("\nüìä –¢–∞–∞–º–∞–≥–ª–∞–ª—ã–Ω —Ç–∞—Ä—Ö–∞–ª—Ç:")
for pred, count in pred_counts.items():
    label = le_target.classes_[pred] if 'le_target' in locals() else str(pred)
    pct = count / len(test_predictions) * 100
    print(f"  {label}: {count} ({pct:.1f}%)")

# “Æ—Ä –¥“Ø–Ω–≥ —Ö–∞–¥–≥–∞–ª–∞—Ö
submission_df = pd.DataFrame()
if test_ids is not None:
    submission_df['Loan_ID'] = test_ids
else:
    submission_df['Loan_ID'] = range(1, len(test_predictions) + 1)

submission_df[target_col] = test_predictions
if 'le_target' in locals():
    submission_df[f'{target_col}_Label'] = le_target.inverse_transform(test_predictions)

submission_df.to_csv('loan_predictions.csv', index=False)
print(f"\n‚úì –¢–∞–∞–º–∞–≥–ª–∞–ª —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: loan_predictions.csv")
print(f"\nüìã –≠—Ö–Ω–∏–π 10 —Ç–∞–∞–º–∞–≥–ª–∞–ª:")
print(submission_df.head(10))

# ======================================================================
# 13. –î“Æ–ì–ù–≠–õ–¢
# ======================================================================

print("\n" + "=" * 80)
print("üìù –ê–õ–•–ê–ú 12: –î“Æ–ì–ù–≠–õ–¢ –ë–ê –ó”®–í–õ”®–ú–ñ")
print("=" * 80)

print(f"""
üéØ –¢”®–°–õ–ò–ô–ù “Æ–† –î“Æ–ù:

1. ”®–ì”®–ì–î–õ–ò–ô–ù –ú–≠–î–≠–≠–õ–≠–õ:
   ‚Ä¢ –°—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª: {len(train_df)} –º”©—Ä
   ‚Ä¢ –¢–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥”©–ª: {len(test_df)} –º”©—Ä
   ‚Ä¢ –®–∏–Ω–∂–∏–π–Ω —Ç–æ–æ: {len(X.columns)}
   ‚Ä¢ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á: {target_col}

2. –•–ê–ú–ì–ò–ô–ù –°–ê–ô–ù –ó–ê–ì–í–ê–†:
   ‚Ä¢ –ó–∞–≥–≤–∞—Ä: {best_model_name}
   ‚Ä¢ –ù–∞—Ä–∏–π–≤—á–ª–∞–ª: {results_df.iloc[0]['accuracy']:.2%}
   ‚Ä¢ F1-Score: {results_df.iloc[0]['f1_score']:.2%}
   ‚Ä¢ Cross-Validation: {results_df.iloc[0]['cv_mean']:.2%} (¬±{results_df.iloc[0]['cv_std']:.2%})
   
3. –•–ê–ú–ì–ò–ô–ù –ß–£–•–ê–õ –•–£–í–¨–°–ê–ì–ß–ò–î (—ç—Ö–Ω–∏–π 5):
{chr(10).join([f'   ‚Ä¢ {row["feature"]}: {row["importance"]:.4f}' 
              for _, row in feature_importance_rf.head(5).iterrows()])}

4. –ë“Æ–•–≠–≠–ì–î–°–≠–ù –î“Æ–ì–ù–≠–õ–¢:
   ‚Ä¢ –ó—ç—ç–ª–∏–π–Ω –±–∞—Ç–ª–∞–º–∂–∏–π–≥ –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞—Ä–≥–∞–∞—Ä {results_df.iloc[0]['accuracy']:.1%} 
     –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª–∞–∞—Ä —Ç–∞–∞–º–∞–≥–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π
   ‚Ä¢ {best_model_name} –∑–∞–≥–≤–∞—Ä —Ö–∞–º–≥–∏–π–Ω —Å–∞–π–Ω “Ø—Ä –¥“Ø–Ω “Ø–∑“Ø“Ø–ª—Å—ç–Ω
   ‚Ä¢ –ë“Ø—Ö –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª {results_df['accuracy'].min():.1%}-{results_df['accuracy'].max():.1%} 
     —Ö–æ–æ—Ä–æ–Ω–¥ –±–∞–π–Ω–∞
   
5. –ü–†–ê–ö–¢–ò–ö–¢ –•–≠–†–≠–ì–õ–≠–•:
   ‚Ä¢ –ë–∞–Ω–∫, —Å–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω –±–∞–π–≥—É—É–ª–ª–∞–≥—É—É–¥–∞–¥ –∑—ç—ç–ª –±–∞—Ç–ª–∞—Ö/—Ç–∞—Ç–≥–∞–ª–∑–∞—Ö —à–∏–π–¥–≤—ç—Ä 
     –≥–∞—Ä–≥–∞—Ö–∞–¥ —Ç—É—Å–ª–∞—Ö
   ‚Ä¢ –≠—Ä—Å–¥—ç–ª–∏–π–Ω “Ø–Ω—ç–ª–≥—ç—ç–≥ –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∞—Ö
   ‚Ä¢ –ó—ç—ç–ª–∏–π–Ω –ø—Ä–æ—Ü–µ—Å—Å—ã–≥ —Ö—É—Ä–¥–∞—Å–≥–∞—Ö
   ‚Ä¢ –•—É–≤—å —Ö“Ø–Ω–∏–π —Ö“Ø—á–∏–Ω –∑“Ø–π–ª–∏–π–≥ –±–∞–≥–∞—Å–≥–∞—Ö
   
6. –¶–ê–ê–®–î–´–ù –°–ê–ô–ñ–†–£–£–õ–ê–õ–¢:
   ‚Ä¢ –ò–ª“Ø“Ø –æ–ª–æ–Ω ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
   ‚Ä¢ Feature engineering - —à–∏–Ω—ç —Ö—É–≤—å—Å–∞–≥—á–∏–¥ “Ø“Ø—Å–≥—ç—Ö
   ‚Ä¢ Hyperparameter tuning - –ø–∞—Ä–∞–º–µ—Ç—Ä—É—É–¥—ã–≥ –æ–Ω–æ–≤—á–∏–ª–æ—Ö
   ‚Ä¢ Ensemble –º–µ—Ç–æ–¥—É—É–¥ —Ç—É—Ä—à–∏–∂ “Ø–∑—ç—Ö
   ‚Ä¢ Deep Learning –∞—Ä–≥—É—É–¥ –∞—à–∏–≥–ª–∞—Ö
   
7. –•–Ø–ó–ì–ê–ê–†–õ–ê–õ–¢:
   ‚Ä¢ ”®–≥”©–≥–¥–ª–∏–π–Ω —á–∞–Ω–∞—Ä, —Ö—ç–º–∂—ç—ç–Ω—ç—ç—Å “Ø—Ä –¥“Ø–Ω —Ö–∞–º–∞–∞—Ä–Ω–∞
   ‚Ä¢ 100% –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π –±–∞–π—Ö –±–æ–ª–æ–º–∂–≥“Ø–π
   ‚Ä¢ –¢–æ–≥—Ç–º–æ–ª —Å—É—Ä–≥–∞–∂, —à–∏–Ω—ç—á–ª—ç—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π
   ‚Ä¢ –ë—É—Å–∞–¥ —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å (—ç–¥–∏–π–Ω –∑–∞—Å–≥–∏–π–Ω –Ω”©—Ö—Ü”©–ª –±–∞–π–¥–∞–ª, 
     –≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫ —ç—Ä—Å–¥—ç–ª) —Ö–∞—Ä–≥–∞–ª–∑–∞—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π
   
‚ö†Ô∏è –ú–≠–†–ì–≠–ñ–õ–ò–ô–ù –Å–° –ó“Æ–ô:
   ‚Ä¢ “Æ—Ä –¥“Ø–Ω–≥ “Ø–Ω—ç–Ω –∑”©–≤, —à—É–¥–∞—Ä–≥–∞–∞—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞—Ö
   ‚Ä¢ –ê–ª–¥–∞–∞, –¥—É—Ç–∞–≥–¥–ª—ã–≥ –Ω—É—É–Ω –¥–∞—Ä–∞–≥–¥—É—É–ª–∞—Ö–≥“Ø–π –±–∞–π—Ö
   ‚Ä¢ –ê—à–∏–≥ —Å–æ–Ω–∏—Ä—Ö–ª—ã–Ω –∑”©—Ä—á–∏–ª–≥”©”©—Å –∞–Ω–≥–∏–¥ –±–∞–π—Ö
   ‚Ä¢ –•—É–≤–∏–π–Ω –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ö–∞–º–≥–∞–∞–ª–∞—Ö
   ‚Ä¢ –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω —à–∏–π–¥–≤—ç—Ä–∏–π–≥ –∑”©–≤—Ö”©–Ω –∑”©–≤–ª”©–º–∂ –±–æ–ª–≥–æ–Ω –∞—à–∏–≥–ª–∞—Ö,
     —ç—Ü—Å–∏–π–Ω —à–∏–π–¥–≤—ç—Ä–∏–π–≥ —Ö“Ø–Ω –≥–∞—Ä–≥–∞—Ö
""")

print("\n" + "=" * 80)
print("‚úÖ –¢”®–°”®–õ –ê–ú–ñ–ò–õ–¢–¢–ê–ô –î–£–£–°–õ–ê–ê!")
print("=" * 80)

print(f"""
 –•–ê–î–ì–ê–õ–ê–ì–î–°–ê–ù –§–ê–ô–õ–£–£–î:
   1. loan_distributions.png - ”®–≥”©–≥–¥–ª–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç
   2. correlation_matrix.png - –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü
   3. feature_relationships.png - –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª
   4. model_comparison.png - –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç
   5. roc_curves.png - ROC –º—É—Ä—É–π (—Ö—ç—Ä—ç–≤ binary classification)
   6. confusion_matrices.png - Confusion matrices
   7. feature_importance.png - –•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª
   8. loan_predictions.csv - –¢–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª
   9. loan_train.csv - –ê–Ω—Ö–Ω—ã —Å—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª (—Ç–∞–Ω–∞–π —Ñ–∞–π–ª)
   10. loan_test.csv - –ê–Ω—Ö–Ω—ã —Ç–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥”©–ª (—Ç–∞–Ω–∞–π —Ñ–∞–π–ª)
    11. app.py - –¢”©—Å–ª–∏–π–Ω –∫–æ–¥
""")
# ======================================================================
# 14. PDF –¢–ê–ô–õ–ê–ù “Æ“Æ–°–ì–≠–•
# ======================================================================

print("\n" + "=" * 80)
print("üìÑ –ê–õ–•–ê–ú 13: PDF –¢–ê–ô–õ–ê–ù “Æ“Æ–°–ì–≠–•")
print("=" * 80)

from matplotlib.backends.backend_pdf import PdfPages
from datetime import datetime

def create_pdf_report():
    """PDF —Ç–∞–π–ª–∞–Ω “Ø“Ø—Å–≥—ç—Ö —Ñ—É–Ω–∫—Ü"""
    
    pdf_filename = f'loan_prediction_report_{datetime.now().strftime("%Y%m%d")}.pdf'
    
    with PdfPages(pdf_filename) as pdf:
        
        # ============ –•–£–£–î–ê–° 1: –ù“Æ“Æ–† –•–£–£–î–ê–° ============
        fig = plt.figure(figsize=(8.5, 11))
        fig.text(0.5, 0.7, '–ó–≠–≠–õ–ò–ô–ù –ë–ê–¢–õ–ê–ú–ñ–ò–ô–ù –¢–ê–ê–ú–ê–ì–õ–ê–õ', 
                ha='center', fontsize=24, fontweight='bold')
        fig.text(0.5, 0.65, '–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Ç”©—Å”©–ª', 
                ha='center', fontsize=16)
        
        fig.text(0.5, 0.55, '–ë–∞–≥–∏–π–Ω –≥–∏—à“Ø“Ø–¥:', 
                ha='center', fontsize=14, fontweight='bold')
        
        team_members = [
            '1. [–ù—ç—Ä 1] - ”®–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞–ª—Ç, —Ü—ç–≤—ç—Ä–ª—ç–≥—ç—ç',
            '2. [–ù—ç—Ä 2] - –ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∑–∞–≥–≤–∞—Ä –∞–∂–∏–ª–ª—É—É–ª–∞–ª—Ç',
            '3. [–ù—ç—Ä 3] - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏, –≥—Ä–∞—Ñ–∏–∫ “Ø“Ø—Å–≥—ç–ª—Ç',
            '4. [–ù—ç—Ä 4] - –î“Ø–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç, –¥“Ø–≥–Ω—ç–ª—Ç',
            '5. [–ù—ç—Ä 5] - –¢–∞–π–ª–∞–Ω –±–∏—á–∏–ª—Ç, —Ç–∞–Ω–∏–ª—Ü—É—É–ª–≥–∞'
        ]
        
        y_pos = 0.48
        for member in team_members:
            fig.text(0.5, y_pos, member, ha='center', fontsize=11)
            y_pos -= 0.04
        
        fig.text(0.5, 0.2, f'–û–≥–Ω–æ–æ: {datetime.now().strftime("%Y-%m-%d")}', 
                ha='center', fontsize=12)
        fig.text(0.5, 0.15, '–≠—Ö —Å—É—Ä–≤–∞–ª–∂: Kaggle - Loan Approval Prediction Dataset', 
                ha='center', fontsize=10, style='italic')
        
        plt.axis('off')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 2: –•–£–†–ê–ê–ù–ì–£–ô ============
        fig = plt.figure(figsize=(8.5, 11))
        fig.text(0.5, 0.95, '–•–£–†–ê–ê–ù–ì–£–ô', ha='center', fontsize=18, fontweight='bold')
        
        summary_text = f"""
–≠–Ω—ç—Ö“Ø“Ø —Ç”©—Å”©–ª –Ω—å –∑—ç—ç–ª–∏–π–Ω –±–∞—Ç–ª–∞–º–∂–∏–π–≥ –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞—Ä–≥–∞–∞—Ä —Ç–∞–∞–º–∞–≥–ª–∞—Ö –∑–æ—Ä–∏–ª–≥–æ—Ç–æ–π.
Kaggle-–∞–∞—Å –∞–≤—Å–∞–Ω {len(train_df)} –º”©—Ä”©”©—Å –±“Ø—Ä–¥—Å—ç–Ω ”©–≥”©–≥–¥–ª–∏–π–≥ –∞—à–∏–≥–ª–∞—Å–∞–Ω.

–ì“Æ–ô–¶–≠–¢–ì–≠–°–≠–ù –ê–ñ–õ–£–£–î:
‚Ä¢ ”®–≥”©–≥–¥–ª–∏–π–Ω —Ü—ç–≤—ç—Ä–ª—ç–ª—Ç, –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞–ª—Ç
‚Ä¢ {len(X.columns)} —Ö—É–≤—å—Å–∞–≥—á–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç
‚Ä¢ 4 —Ç”©—Ä–ª–∏–π–Ω –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∑–∞–≥–≤–∞—Ä –∞–∂–∏–ª–ª—É—É–ª—Å–∞–Ω
‚Ä¢ –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω

“Æ–† –î“Æ–ù:
–•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∑–∞–≥–≤–∞—Ä: {best_model_name}
–ù–∞—Ä–∏–π–≤—á–ª–∞–ª: {results_df.iloc[0]['accuracy']:.2%}
F1-Score: {results_df.iloc[0]['f1_score']:.2%}

–î“Æ–ì–ù–≠–õ–¢:
–ó—ç—ç–ª–∏–π–Ω –±–∞—Ç–ª–∞–º–∂–∏–π–≥ {results_df.iloc[0]['accuracy']:.1%} –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª–∞–∞—Ä 
—Ç–∞–∞–º–∞–≥–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π –±–æ–ª—Å–æ–Ω. –≠–Ω—ç –Ω—å –±–∞–Ω–∫, —Å–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω –±–∞–π–≥—É—É–ª–ª–∞–≥—É—É–¥–∞–¥
–∑—ç—ç–ª –æ–ª–≥–æ—Ö —à–∏–π–¥–≤—ç—Ä –≥–∞—Ä–≥–∞—Ö–∞–¥ —Ç—É—Å–∞–ª–Ω–∞.
        """
        
        fig.text(0.1, 0.85, summary_text, fontsize=11, verticalalignment='top',
                wrap=True)
        
        plt.axis('off')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 3: ”®–ì”®–ì–î–õ–ò–ô–ù –¢–ê–†–•–ê–õ–¢ ============
        img = plt.imread('loan_distributions.png')
        fig = plt.figure(figsize=(8.5, 11))
        plt.imshow(img)
        plt.axis('off')
        plt.title('”®–≥”©–≥–¥–ª–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç', fontsize=16, fontweight='bold', pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 4: –ö–û–†–†–ï–õ–Ø–¶–ò ============
        img = plt.imread('correlation_matrix.png')
        fig = plt.figure(figsize=(8.5, 11))
        plt.imshow(img)
        plt.axis('off')
        plt.title('–•—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∫–æ—Ä—Ä–µ–ª—è—Ü–∏', fontsize=16, fontweight='bold', pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 5: –ó–ê–ì–í–ê–†–£–£–î–´–ù –•–ê–†–¨–¶–£–£–õ–ê–õ–¢ ============
        img = plt.imread('model_comparison.png')
        fig = plt.figure(figsize=(8.5, 11))
        plt.imshow(img)
        plt.axis('off')
        plt.title('–ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç', fontsize=16, fontweight='bold', pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 6: CONFUSION MATRICES ============
        img = plt.imread('confusion_matrices.png')
        fig = plt.figure(figsize=(8.5, 11))
        plt.imshow(img)
        plt.axis('off')
        plt.title('Confusion Matrices', fontsize=16, fontweight='bold', pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 7: –•–£–í–¨–°–ê–ì–ß–î–´–ù –ê–ß –•–û–õ–ë–û–ì–î–û–õ ============
        img = plt.imread('feature_importance.png')
        fig = plt.figure(figsize=(8.5, 11))
        plt.imshow(img)
        plt.axis('off')
        plt.title('–•—É–≤—å—Å–∞–≥—á–¥—ã–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª', fontsize=16, fontweight='bold', pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 8: “Æ–† –î“Æ–ù–ì–ò–ô–ù –•“Æ–°–ù–≠–ì–¢ ============
        fig = plt.figure(figsize=(8.5, 11))
        fig.text(0.5, 0.95, '–ó–ê–ì–í–ê–†–£–£–î–´–ù “Æ–† –î“Æ–ù–ì–ò–ô–ù –•“Æ–°–ù–≠–ì–¢', 
                ha='center', fontsize=16, fontweight='bold')
        
        # –•“Ø—Å–Ω—ç–≥—Ç “Ø“Ø—Å–≥—ç—Ö
        ax = fig.add_subplot(111)
        ax.axis('tight')
        ax.axis('off')
        
        table_data = []
        table_data.append(['–ó–∞–≥–≤–∞—Ä', 'Accuracy', 'F1-Score', 'AUC', 'CV Score'])
        
        for idx, row in results_df.iterrows():
            table_data.append([
                idx,
                f"{row['accuracy']:.4f}",
                f"{row['f1_score']:.4f}",
                f"{row['auc']:.4f}",
                f"{row['cv_mean']:.4f} (¬±{row['cv_std']:.4f})"
            ])
        
        table = ax.table(cellText=table_data, cellLoc='center', loc='center',
                        colWidths=[0.3, 0.15, 0.15, 0.15, 0.25])
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)
        
        # Header-–∏–π–≥ —Ç–æ–¥—Ä—É—É–ª–∞—Ö
        for i in range(5):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 9: –î“Æ–ì–ù–≠–õ–¢ ============
        fig = plt.figure(figsize=(8.5, 11))
        fig.text(0.5, 0.95, '–î“Æ–ì–ù–≠–õ–¢ –ë–ê –ó”®–í–õ”®–ú–ñ', 
                ha='center', fontsize=18, fontweight='bold')
        
        conclusion_text = f"""
1. ”®–ì”®–ì–î–õ–ò–ô–ù –ú–≠–î–≠–≠–õ–≠–õ:
   ‚Ä¢ –°—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª: {len(train_df)} –º”©—Ä
   ‚Ä¢ –®–∏–Ω–∂–∏–π–Ω —Ç–æ–æ: {len(X.columns)}
   ‚Ä¢ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á: {target_col}

2. –•–ê–ú–ì–ò–ô–ù –°–ê–ô–ù –ó–ê–ì–í–ê–†:
   ‚Ä¢ –ó–∞–≥–≤–∞—Ä: {best_model_name}
   ‚Ä¢ –ù–∞—Ä–∏–π–≤—á–ª–∞–ª: {results_df.iloc[0]['accuracy']:.2%}
   ‚Ä¢ F1-Score: {results_df.iloc[0]['f1_score']:.2%}

3. –•–ê–ú–ì–ò–ô–ù –ß–£–•–ê–õ –•–£–í–¨–°–ê–ì–ß–ò–î:
"""
        for _, row in feature_importance_rf.head(5).iterrows():
            conclusion_text += f"   ‚Ä¢ {row['feature']}: {row['importance']:.4f}\n"
        
        conclusion_text += f"""

4. –î“Æ–ì–ù–≠–õ–¢:
   –ó—ç—ç–ª–∏–π–Ω –±–∞—Ç–ª–∞–º–∂–∏–π–≥ {results_df.iloc[0]['accuracy']:.1%} –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª–∞–∞—Ä 
   —Ç–∞–∞–º–∞–≥–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π –±–æ–ª—Å–æ–Ω. {best_model_name} –∑–∞–≥–≤–∞—Ä —Ö–∞–º–≥–∏–π–Ω 
   —Å–∞–π–Ω “Ø—Ä –¥“Ø–Ω “Ø–∑“Ø“Ø–ª—Å—ç–Ω.

5. –ü–†–ê–ö–¢–ò–ö–¢ –•–≠–†–≠–ì–õ–≠–•:
   ‚Ä¢ –ë–∞–Ω–∫–Ω—ã –∑—ç—ç–ª –±–∞—Ç–ª–∞—Ö/—Ç–∞—Ç–≥–∞–ª–∑–∞—Ö —à–∏–π–¥–≤—ç—Ä—Ç —Ç—É—Å–ª–∞—Ö
   ‚Ä¢ –≠—Ä—Å–¥—ç–ª–∏–π–Ω “Ø–Ω—ç–ª–≥—ç—ç–≥ –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∞—Ö
   ‚Ä¢ –ó—ç—ç–ª–∏–π–Ω –ø—Ä–æ—Ü–µ—Å—Å—ã–≥ —Ö—É—Ä–¥–∞—Å–≥–∞—Ö

6. –¶–ê–ê–®–î–´–ù –°–ê–ô–ñ–†–£–£–õ–ê–õ–¢:
   ‚Ä¢ –ò–ª“Ø“Ø –æ–ª–æ–Ω ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
   ‚Ä¢ Feature engineering —Ö–∏–π—Ö
   ‚Ä¢ Hyperparameter tuning —Ö–∏–π—Ö
   ‚Ä¢ Ensemble –º–µ—Ç–æ–¥—É—É–¥ —Ç—É—Ä—à–∏–∂ “Ø–∑—ç—Ö
"""
        
        fig.text(0.1, 0.85, conclusion_text, fontsize=10, verticalalignment='top',
                family='monospace')
        
        plt.axis('off')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # ============ –•–£–£–î–ê–° 10: –≠–® –°–£–†–í–ê–õ–ñ ============
        fig = plt.figure(figsize=(8.5, 11))
        fig.text(0.5, 0.95, '–ê–®–ò–ì–õ–ê–°–ê–ù –ú–ê–¢–ï–†–ò–ê–õ–´–ù –ñ–ê–ì–°–ê–ê–õ–¢', 
                ha='center', fontsize=18, fontweight='bold')
        
        references = """
–≠–®–õ–≠–õ (APA –§–û–†–ú–ê–¢):

[1] Kaggle. (2024). Loan Approval Prediction Dataset. 
    Retrieved from https://www.kaggle.com/datasets/

[2] Scikit-learn Developers. (2024). Scikit-learn: Machine Learning in Python.
    Retrieved from https://scikit-learn.org/

[3] Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python.
    Journal of Machine Learning Research, 12, 2825-2830.

[4] McKinney, W. (2010). Data Structures for Statistical Computing in Python.
    Proceedings of the 9th Python in Science Conference, 56-61.

[5] Hunter, J. D. (2007). Matplotlib: A 2D Graphics Environment.
    Computing in Science & Engineering, 9(3), 90-95.

[6] Waskom, M. (2021). seaborn: statistical data visualization.
    Journal of Open Source Software, 6(60), 3021.


–ê–®–ò–ì–õ–ê–°–ê–ù –ü–†–û–ì–†–ê–ú –•–ê–ù–ì–ê–ú–ñ:

‚Ä¢ Python 3.8+
‚Ä¢ pandas 1.3.0+
‚Ä¢ numpy 1.21.0+
‚Ä¢ matplotlib 3.4.0+
‚Ä¢ seaborn 0.11.0+
‚Ä¢ scikit-learn 1.0.0+


”®–ì”®–ì–î–õ–ò–ô–ù –≠–® –°–£–†–í–ê–õ–ñ:

–¢”©—Å–ª–∏–π–Ω –∞–∂–∏–ª–¥ –∞—à–∏–≥–ª–∞—Å–∞–Ω ”©–≥”©–≥–¥”©–ª –Ω—å Kaggle –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥—ç—ç—Ä—Ö
"Loan Approval Prediction Dataset" —é–º. –≠–Ω—ç ”©–≥”©–≥–¥”©–ª –Ω—å –∑—ç—ç–ª –∞–≤–∞–≥—á–¥—ã–Ω
—Ö—É–≤–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª, –æ—Ä–ª–æ–≥–æ, ”©—Ä —Ç”©–ª–±”©—Ä –±–æ–ª–æ–Ω –±—É—Å–∞–¥ —Å–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω 
–º—ç–¥—ç—ç–ª–ª“Ø“Ø–¥–∏–π–≥ –∞–≥—É—É–ª–¥–∞–≥.
"""
        
        fig.text(0.1, 0.85, references, fontsize=9, verticalalignment='top',
                family='monospace')
        
        plt.axis('off')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()
        
        # PDF metadata –Ω—ç–º—ç—Ö
        d = pdf.infodict()
        d['Title'] = '–ó—ç—ç–ª–∏–π–Ω –±–∞—Ç–ª–∞–º–∂–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª - –ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Ç”©—Å”©–ª'
        d['Author'] = '–ë–∞–≥–∏–π–Ω –≥–∏—à“Ø“Ø–¥'
        d['Subject'] = '–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç, –ó—ç—ç–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª'
        d['Keywords'] = 'Machine Learning, Loan Prediction, Credit Risk'
        d['CreationDate'] = datetime.now()
    
    return pdf_filename

# PDF “Ø“Ø—Å–≥—ç—Ö
try:
    pdf_file = create_pdf_report()
    print(f"\nPDF —Ç–∞–π–ª–∞–Ω –∞–º–∂–∏–ª—Ç—Ç–∞–π “Ø“Ø—Å–≥—ç–≥–¥–ª—ç—ç: {pdf_file}")
except Exception as e:
    print(f"\n–ê–õ–î–ê–ê: PDF “Ø“Ø—Å–≥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")

print("\n" + "=" * 80)
print("–ë“Æ–• –ê–ñ–ò–õ –î–£–£–°–õ–ê–ê!")
print("=" * 80)
